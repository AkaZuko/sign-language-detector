{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "from skimage.transform import resize, pyramid_gaussian\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import svm\n",
    "from skimage.feature import hog\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GestureRecognizer(object):\n",
    "\n",
    "    \"\"\"class to perform gesture recognition\"\"\"\n",
    "\n",
    "    def __init__(self, data_directory):\n",
    "\n",
    "        \"\"\"\n",
    "            data_directory : path like /home/sanket/mlproj/dataset/\n",
    "            includes the dataset folder with '/'\n",
    "\n",
    "            Initialize all your variables here\n",
    "        \"\"\"\n",
    "        self.base_dir = data_directory\n",
    "        # self.base_dir = os.path.abspath('.') + '/dataset/'\n",
    "        self.win_size = 128\n",
    "        self.clf_gesture = None\n",
    "        self.clf_hnh = None\n",
    "\n",
    "    def IOU(self, A, B):\n",
    "        x_overlap = max(0, min(B[0],B[2]) - max(A[0],A[2]))\n",
    "        y_overlap = max(0, min(B[1],B[3]) - max(A[1],A[3]))\n",
    "        inter = x_overlap * y_overlap;\n",
    "        \n",
    "        A_area = (A[2] - A[0] + 1)*(A[3] - A[1] + 1)\n",
    "        B_area = (B[2] - B[0] + 1)*(B[3] - B[1] + 1)\n",
    "        \n",
    "        union = (A_area + B_area - inter)*1.0\n",
    "        inter = inter*1.0\n",
    "        \n",
    "        return inter/union\n",
    "    \n",
    "    def train(self, train_list):\n",
    "\n",
    "        \"\"\"\n",
    "            train_list : list of users to use for training\n",
    "            eg [\"user_1\", \"user_2\", \"user_3\"]\n",
    "\n",
    "            The train function should train all your classifiers,\n",
    "            both binary and multiclass on the given list of users\n",
    "        \"\"\"\n",
    "        \n",
    "        train_x_pos_ = []\n",
    "        train_x_neg_ = []\n",
    "        train_y = []\n",
    "        \n",
    "        for user in train_list:\n",
    "            csv_file = self.base_dir + user + '/' + user + '_loc.csv'\n",
    "            with open(csv_file,'r') as f:\n",
    "                f.readline()\n",
    "                for line in f:\n",
    "                    data = line.strip().split(',')\n",
    "                    file_name = data[0]\n",
    "                    x1,y1,x2,y2 = map(int, data[1:])\n",
    "                    \n",
    "                    img = io.imread(self.base_dir + file_name,as_grey=True)\n",
    "                    h,w = img.shape[:2]\n",
    "                    imgg = img[y1:y2,x1:x2]\n",
    "                    imgg = resize(imgg, (self.win_size, self.win_size))\n",
    "                    imgg_hog = hog(imgg)\n",
    "                    \n",
    "                    label = ord(file_name.split('/')[1][0])\n",
    "                    \n",
    "                    train_x_pos_.append(imgg_hog)\n",
    "                    train_y.append(label)\n",
    "                    \n",
    "                    count = 0\n",
    "                    \n",
    "                    A = [x1,y1,x2,y2]\n",
    "                    \n",
    "                    while True:\n",
    "                        x1_r = random.randrange(0,w - self.win_size)\n",
    "                        y1_r = random.randrange(0,h - self.win_size)\n",
    "                        x2_r = x1_r + self.win_size\n",
    "                        y2_r = y1_r + self.win_size\n",
    "                        \n",
    "                        if y2_r >= h or x2_r>=w:\n",
    "                            continue\n",
    "                        \n",
    "                        B = [x1_r, y1_r, x2_r, y2_r]\n",
    "                        \n",
    "                        if self.IOU(A,B) < 0.1:\n",
    "                            train_x_neg_.append(hog(img[y1_r:y2_r,x1_r:x2_r]))\n",
    "                            count += 1\n",
    "                        \n",
    "                        if count >= 2:\n",
    "                            break\n",
    "        \n",
    "        self.train_x_pos = np.asarray(train_x_pos_)\n",
    "        del train_x_pos_\n",
    "        self.train_x_neg = np.asarray(train_x_neg_)\n",
    "        del train_x_neg_\n",
    "        train_y = np.asarray(train_y)\n",
    "        \n",
    "        self.clf_gesture =  svm.LinearSVC()\n",
    "        self.clf_gesture.fit(self.train_x_pos, train_y)\n",
    "        score_gesture = self.clf_gesture.score(self.train_x_pos, train_y)\n",
    "        print 'Training accuracy for gesture classifier : %f' %(score_gesture)\n",
    "        \n",
    "#         self.clf_hnh = svm.LinearSVC()\n",
    "        self.clf_hnh = GaussianNB()\n",
    "        train_x_hnh = np.concatenate((self.train_x_pos , self.train_x_neg))\n",
    "        train_y_hnh = np.asarray( [1] * len(self.train_x_pos) + [0] * len(self.train_x_neg))\n",
    "        self.clf_hnh.partial_fit(train_x_hnh, train_y_hnh, classes = np.asarray([0,1]))\n",
    "        score_hnh = self.clf_hnh.score(train_x_hnh, train_y_hnh)\n",
    "        print 'Training accuracy for Hand/Non-hand classifier : %f' %(score_hnh)\n",
    "    \n",
    "    \n",
    "    def test(self, test_list):\n",
    "\n",
    "        \"\"\"\n",
    "            train_list : list of users to use for training\n",
    "            eg [\"user_1\", \"user_2\", \"user_3\"]\n",
    "\n",
    "            The train function should train all your classifiers,\n",
    "            both binary and multiclass on the given list of users\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.clf_hnh is None or self.clf_gesture is None:\n",
    "            print 'Classifiers not trained'\n",
    "            return\n",
    "        \n",
    "        test_x_pos = []\n",
    "        test_x_neg = []\n",
    "        test_y = []\n",
    "        \n",
    "        for user in test_list:\n",
    "            csv_file = self.base_dir + user + '/' + user + '_loc.csv'\n",
    "            with open(csv_file,'r') as f:\n",
    "                f.readline()\n",
    "                for line in f:\n",
    "                    data = line.strip().split(',')\n",
    "                    file_name = data[0]\n",
    "                    x1,y1,x2,y2 = map(int, data[1:])\n",
    "                    \n",
    "                    img = io.imread(self.base_dir + file_name,as_grey=True)\n",
    "                    h,w = img.shape[:2]\n",
    "                    imgg = img[y1:y2,x1:x2]\n",
    "                    imgg = resize(imgg, (self.win_size, self.win_size))\n",
    "                    imgg_hog = hog(imgg)\n",
    "                    \n",
    "                    label = ord(file_name.split('/')[1][0])\n",
    "                    \n",
    "                    test_x_pos.append(imgg_hog)\n",
    "                    test_y.append(label)\n",
    "                    \n",
    "                    count = 0\n",
    "                    \n",
    "                    A = [x1,y1,x2,y2]\n",
    "                    \n",
    "                    while True:\n",
    "                        x1_r = random.randrange(0,w - self.win_size)\n",
    "                        y1_r = random.randrange(0,h - self.win_size)\n",
    "                        x2_r = x1_r + self.win_size\n",
    "                        y2_r = y1_r + self.win_size\n",
    "                        \n",
    "                        if y2_r >= h or x2_r>=w:\n",
    "                            continue\n",
    "                        \n",
    "                        B = [x1_r, y1_r, x2_r, y2_r]\n",
    "                        \n",
    "                        if self.IOU(A,B) < 0.1:\n",
    "                            test_x_neg.append(hog(img[y1_r:y2_r,x1_r:x2_r]))\n",
    "                            count += 1\n",
    "                        \n",
    "                        if count >= 2:\n",
    "                            break\n",
    "        \n",
    "        \n",
    "        test_x_pos = np.asarray(test_x_pos)\n",
    "        test_x_neg = np.asarray(test_x_neg)\n",
    "        test_y = np.asarray(test_y)\n",
    "        \n",
    "        score_gesture = self.clf_gesture.score(test_x_pos, test_y)\n",
    "        print 'Testing accuracy for gesture classifier : %f' %(score_gesture)\n",
    "        \n",
    "        \n",
    "        test_x_hnh = np.concatenate((test_x_pos, test_x_neg))\n",
    "        test_y_hnh = np.asarray( [1] * len(test_x_pos) + [0] * len(test_x_neg) )\n",
    "        \n",
    "        score_hnh = self.clf_hnh.score(test_x_hnh,test_y_hnh)\n",
    "        print 'Testing accuracy for Hand/Non-hand classifier : %f' %(score_hnh)\n",
    "\n",
    "    \n",
    "    def hard_negative_mining(self, no_iter, threshold):\n",
    "        \n",
    "        if self.clf_hnh is None or self.clf_gesture is None:\n",
    "            print 'Classifiers not trained'\n",
    "            return\n",
    "        \n",
    "        for i in xrange(no_iter):\n",
    "            count = 0\n",
    "            FP = []\n",
    "            \n",
    "            for data in self.train_x_neg:\n",
    "                if self.clf_hnh.predict([data])[0] == 1:\n",
    "                    count+=1\n",
    "                    FP.append(data)\n",
    "            \n",
    "            print count\n",
    "            if count <= threshold:\n",
    "                break\n",
    "            \n",
    "            self.clf_hnh.partial_fit(np.asarray(FP), np.asarray([0] * len(FP)))\n",
    "        \n",
    "        Y = np.asarray([1] * self.train_x_pos.shape[0] +  [0] * self.train_x_neg.shape[0])\n",
    "        score_ = self.clf_hnh.score(np.concatenate((self.train_x_pos, self.train_x_neg)), Y)\n",
    "        print 'Accuracy for Hand/Non-hand classifier after Hard Negative Mining : %f' %(score_)\n",
    "        \n",
    "    def sliding_window(self, img, clf):\n",
    "        conf_map = np.zeros(img.shape)\n",
    "        h,w = img.shape[:2]\n",
    "        stride = 10\n",
    "        win_size = 128\n",
    "        for y in range(0,h-win_size+1,stride):\n",
    "            for x in range(0,w-win_size+1,stride):\n",
    "                imgg = img[y:y+win_size, x:x+win_size]\n",
    "                hog_ = hog(imgg)\n",
    "                class_ = clf.predict_proba(np.asarray([hog_]))[0]\n",
    "                for i in range(128):\n",
    "                    for j in range(128):\n",
    "                        conf_map[y + i][x + j] = max(conf_map[y + i][x + j], class_[1])\n",
    "        return conf_map\n",
    "    \n",
    "    \n",
    "    def get_bbox(self, image):\n",
    "        \n",
    "        if self.clf_hnh is None or self.clf_gesture is None:\n",
    "            print 'Classifiers not trained'\n",
    "            return\n",
    "        \n",
    "        py = pyramid_gaussian(image, downscale=1.3)\n",
    "        py_img  = [py.next(), py.next(), py.next()]\n",
    "        conf_maps = map(lambda x : self.sliding_window(x, self.clf_hnh), py_img)\n",
    "        max_val = max(map(lambda x: x.max(), conf_maps))\n",
    "        \n",
    "        for i in xrange(len(conf_maps)):\n",
    "            if conf_maps[i].max() == max_val:\n",
    "                cmap = resize(conf_maps[i], image.shape)\n",
    "                a,b = np.where(cmap==cmap.max())\n",
    "                return [a[0],b[0],a[-1], b[-1]]\n",
    "        \n",
    "        return []\n",
    "        \n",
    "    def recognize_gesture(self, image):\n",
    "\n",
    "        \"\"\"\n",
    "            image : a 320x240 pixel RGB image in the form of a numpy array\n",
    "            \n",
    "            This function should locate the hand and classify the gesture.\n",
    "\n",
    "            returns : (position, label)\n",
    "\n",
    "            position : a tuple of (x1,y1,x2,y2) coordinates of bounding box\n",
    "                       x1,y1 is top left corner, x2,y2 is bottom right\n",
    "\n",
    "            label : a single character. eg 'A' or 'B'\n",
    "        \"\"\"\n",
    "        position = self.get_bbox(image)\n",
    "        imgg = image[position[0]:position[2], position[1] : position[3]]\n",
    "        imgg = resize(imgg, (self.win_size, self.win_size))\n",
    "        label = chr(self.clf_gesture.predict(np.asarray( [hog(imgg)]))[0])\n",
    "        return position, label\n",
    "\n",
    "    def translate_video(self, image_array):\n",
    "\n",
    "        \"\"\"\n",
    "            image_array : a list of images as described above.\n",
    "                          can be of arbitrary length\n",
    "\n",
    "            This function classifies the video into a 5 character string\n",
    "\n",
    "            returns : word (a string of 5 characters)\n",
    "                    no two consecutive characters are identical\n",
    "        \"\"\"\n",
    "\n",
    "        return word\n",
    "    \n",
    "    def test_labelled_images(self):\n",
    "        \n",
    "        if self.clf_hnh is None or self.clf_gesture is None:\n",
    "            print 'Classifiers not trained'\n",
    "            return\n",
    "\n",
    "        test_list = [3,4,5,6,7,9,10,11,12,13,14,15,16,17,18,19]\n",
    "        test_list = map(lambda x : 'user_' + str(x), test_list)\n",
    "        \n",
    "        count = 0\n",
    "        total = 0\n",
    "        for user in test_list:\n",
    "            csv_file = self.base_dir + user + '/' + user + '_loc.csv'\n",
    "            with open(csv_file,'r') as f:\n",
    "                f.readline()\n",
    "                for line in f:\n",
    "                    data = line.strip().split(',')\n",
    "                    file_name = data[0]\n",
    "                    x1,y1,x2,y2 = map(int, data[1:])\n",
    "                    \n",
    "                    img = io.imread(self.base_dir + file_name,as_grey=True)\n",
    "                    total+=1\n",
    "                    h,w = img.shape[:2]\n",
    "                    label = file_name.split('/')[1][0]\n",
    "                    \n",
    "                    pos, label_ = self.recognize_gesture(img)\n",
    "                    \n",
    "                    if label == label_:\n",
    "                        count+=1\n",
    "        \n",
    "        score_gesture = (count*1.0)/(total*1.0)\n",
    "        print 'Testing accuracy for gesture classifier class : %f' %(score_gesture)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ayush/GPU_ML/ML/workspace/project/complete_data/dataset/\n"
     ]
    }
   ],
   "source": [
    "G = GestureRecognizer(os.path.abspath('.') + '/dataset/')\n",
    "print G.base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user_3', 'user_4', 'user_5', 'user_6', 'user_7', 'user_9', 'user_10', 'user_11', 'user_12', 'user_13', 'user_14', 'user_15']\n",
      "['user_16', 'user_17', 'user_18', 'user_19']\n"
     ]
    }
   ],
   "source": [
    "user = [3,4,5,6,7,9,10,11,12,13,14,15]\n",
    "user = map(lambda x : 'user_' + str(x), user)\n",
    "print user\n",
    "\n",
    "user_test = [16,17,18,19]\n",
    "user_test = map(lambda x : 'user_' + str(x), user_test)\n",
    "print user_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for gesture classifier : 1.000000\n",
      "Training accuracy for Hand/Non-hand classifier : 0.902778\n"
     ]
    }
   ],
   "source": [
    "G.train(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "711\n",
      "533\n",
      "446\n",
      "401\n",
      "360\n",
      "326\n",
      "312\n",
      "288\n",
      "279\n",
      "249\n",
      "227\n",
      "212\n",
      "196\n",
      "180\n",
      "179\n",
      "161\n",
      "159\n",
      "152\n",
      "150\n",
      "138\n",
      "133\n",
      "133\n",
      "122\n",
      "113\n",
      "106\n",
      "104\n",
      "100\n",
      "95\n",
      "87\n",
      "85\n",
      "79\n",
      "75\n",
      "72\n",
      "69\n",
      "64\n",
      "70\n",
      "65\n",
      "58\n",
      "61\n",
      "61\n",
      "54\n",
      "53\n",
      "50\n",
      "47\n",
      "45\n",
      "38\n",
      "40\n",
      "42\n",
      "37\n",
      "34\n",
      "Accuracy for Hand/Non-hand classifier after Hard Negative Mining : 0.965162\n"
     ]
    }
   ],
   "source": [
    "G.hard_negative_mining(50,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy for gesture classifier : 0.764583\n",
      "Testing accuracy for Hand/Non-hand classifier : 0.921875\n"
     ]
    }
   ],
   "source": [
    "G.test(user_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clf_hnh_good_957.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(G.clf_hnh, 'clf_hnh_good_957.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Buffer not C contiguous.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-43d577bd9848>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_labelled_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-fa3e0e55a0cf>\u001b[0m in \u001b[0;36mtest_labelled_images\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecognize_gesture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-fa3e0e55a0cf>\u001b[0m in \u001b[0;36mrecognize_gesture\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_bbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mimgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mimgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwin_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwin_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf_gesture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ayush/GPU_ML/ML/lib/python2.7/site-packages/skimage/transform/_warps.pyc\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range)\u001b[0m\n\u001b[1;32m    117\u001b[0m         out = warp(image, tform, output_shape=output_shape, order=order,\n\u001b[1;32m    118\u001b[0m                    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    preserve_range=preserve_range)\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ayush/GPU_ML/ML/lib/python2.7/site-packages/skimage/transform/_geometric.pyc\u001b[0m in \u001b[0;36mwarp\u001b[0;34m(image, inverse_map, map_args, output_shape, order, mode, cval, clip, preserve_range)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                 warped = _warp_fast(image, matrix,\n\u001b[1;32m   1342\u001b[0m                                  \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m                                  order=order, mode=mode, cval=cval)\n\u001b[0m\u001b[1;32m   1344\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m                 \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mskimage/transform/_warps_cy.pyx\u001b[0m in \u001b[0;36mskimage.transform._warps_cy._warp_fast (skimage/transform/_warps_cy.c:2272)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Buffer not C contiguous."
     ]
    }
   ],
   "source": [
    "G.test_labelled_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
