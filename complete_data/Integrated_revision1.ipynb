{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from skimage import io\n",
    "from skimage.transform import resize, pyramid_gaussian\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import svm\n",
    "from skimage.feature import hog\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import os\n",
    "from sklearn.externals import joblib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GestureRecognizer(object):\n",
    "\n",
    "    \"\"\"class to perform gesture recognition\"\"\"\n",
    "\n",
    "    def __init__(self, data_directory):\n",
    "\n",
    "        \"\"\"\n",
    "            data_directory : path like /home/sanket/mlproj/dataset/\n",
    "            includes the dataset folder with '/'\n",
    "\n",
    "            Initialize all your variables here\n",
    "        \"\"\"\n",
    "        self.base_dir = data_directory\n",
    "        # self.base_dir = os.path.abspath('.') + '/dataset/'\n",
    "        self.win_size = 128\n",
    "        self.clf_gesture = None\n",
    "        self.clf_hnh = None\n",
    "\n",
    "    def IOU(self,boxA, boxB):\n",
    "        xA = max(boxA[0], boxB[0])\n",
    "        yA = max(boxA[1], boxB[1])\n",
    "        xB = min(boxA[2], boxB[2])\n",
    "        yB = min(boxA[3], boxB[3])\n",
    "\n",
    "        # compute the area of intersection rectangle\n",
    "        interArea = (xB - xA + 1) * (yB - yA + 1)\n",
    "\n",
    "        # compute the area of both the prediction and ground-truth\n",
    "        # rectangles\n",
    "        boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "        boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "        iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "        # return the intersection over union value\n",
    "        return iou\n",
    "    \n",
    "#     def IOU(self, A, B):\n",
    "#         x_overlap = max(0, min(B[0],B[2]) - max(A[0],A[2]))\n",
    "#         y_overlap = max(0, min(B[1],B[3]) - max(A[1],A[3]))\n",
    "#         inter = x_overlap * y_overlap;\n",
    "        \n",
    "#         A_area = (A[2] - A[0] + 1)*(A[3] - A[1] + 1)\n",
    "#         B_area = (B[2] - B[0] + 1)*(B[3] - B[1] + 1)\n",
    "        \n",
    "#         union = (A_area + B_area - inter)*1.0\n",
    "#         inter = inter*1.0\n",
    "        \n",
    "#         return inter/union\n",
    "    \n",
    "    def train(self, train_list):\n",
    "\n",
    "        \"\"\"\n",
    "            train_list : list of users to use for training\n",
    "            eg [\"user_1\", \"user_2\", \"user_3\"]\n",
    "\n",
    "            The train function should train all your classifiers,\n",
    "            both binary and multiclass on the given list of users\n",
    "        \"\"\"\n",
    "        \n",
    "        train_x_pos_ = []\n",
    "        train_x_neg_ = []\n",
    "        train_y = []\n",
    "        \n",
    "        for user in train_list:\n",
    "            csv_file = self.base_dir + user + '/' + user + '_loc.csv'\n",
    "            with open(csv_file,'r') as f:\n",
    "                f.readline()\n",
    "                for line in f:\n",
    "                    data = line.strip().split(',')\n",
    "                    file_name = data[0]\n",
    "                    x1,y1,x2,y2 = map(int, data[1:])\n",
    "                    \n",
    "                    img = io.imread(self.base_dir + file_name,as_grey=True)\n",
    "                    h,w = img.shape[:2]\n",
    "                    imgg = img[y1:y2,x1:x2]\n",
    "                    imgg = resize(imgg, (self.win_size, self.win_size))\n",
    "                    imgg_hog = hog(imgg)\n",
    "                    \n",
    "                    label = ord(file_name.split('/')[1][0])\n",
    "                    \n",
    "                    train_x_pos_.append(imgg_hog)\n",
    "                    train_y.append(label)\n",
    "                    \n",
    "                    count = 0\n",
    "                    \n",
    "                    A = [x1,y1,x2,y2]\n",
    "                    \n",
    "                    while True:\n",
    "                        x1_r = random.randrange(0,w - self.win_size)\n",
    "                        y1_r = random.randrange(0,h - self.win_size)\n",
    "                        x2_r = x1_r + self.win_size\n",
    "                        y2_r = y1_r + self.win_size\n",
    "                        \n",
    "                        if y2_r >= h or x2_r>=w:\n",
    "                            continue\n",
    "                        \n",
    "                        B = [x1_r, y1_r, x2_r, y2_r]\n",
    "                        \n",
    "                        if self.IOU(A,B) <= 0.5:\n",
    "                            train_x_neg_.append(hog(img[y1_r:y2_r,x1_r:x2_r]))\n",
    "                            count += 1\n",
    "                        \n",
    "                        if count >= 2:\n",
    "                            break\n",
    "        \n",
    "        self.train_x_pos = np.asarray(train_x_pos_)\n",
    "        del train_x_pos_\n",
    "        self.train_x_neg = np.asarray(train_x_neg_)\n",
    "        del train_x_neg_\n",
    "        train_y = np.asarray(train_y)\n",
    "        \n",
    "        self.clf_gesture =  svm.LinearSVC()\n",
    "        self.clf_gesture.fit(self.train_x_pos, train_y)\n",
    "        score_gesture = self.clf_gesture.score(self.train_x_pos, train_y)\n",
    "        print 'Training accuracy for gesture classifier : %f' %(score_gesture)\n",
    "        \n",
    "        # self.clf_hnh = svm.LinearSVC()\n",
    "        self.clf_hnh = GaussianNB()\n",
    "        train_x_hnh = np.concatenate((self.train_x_pos , self.train_x_neg))\n",
    "        train_y_hnh = np.asarray( [1] * len(self.train_x_pos) + [0] * len(self.train_x_neg))\n",
    "        self.clf_hnh.partial_fit(train_x_hnh, train_y_hnh, classes = np.asarray([1,0]))\n",
    "        score_hnh = self.clf_hnh.score(train_x_hnh, train_y_hnh)\n",
    "        print 'Training accuracy for Hand/Non-hand classifier : %f' %(score_hnh)\n",
    "    \n",
    "    \n",
    "    def test(self, test_list):\n",
    "\n",
    "        \"\"\"\n",
    "            train_list : list of users to use for training\n",
    "            eg [\"user_1\", \"user_2\", \"user_3\"]\n",
    "\n",
    "            The train function should train all your classifiers,\n",
    "            both binary and multiclass on the given list of users\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.clf_hnh is None or self.clf_gesture is None:\n",
    "            print 'Classifiers not trained'\n",
    "            return\n",
    "        \n",
    "        test_x_pos = []\n",
    "        test_x_neg = []\n",
    "        test_y = []\n",
    "        \n",
    "        for user in test_list:\n",
    "            csv_file = self.base_dir + user + '/' + user + '_loc.csv'\n",
    "            with open(csv_file,'r') as f:\n",
    "                f.readline()\n",
    "                for line in f:\n",
    "                    data = line.strip().split(',')\n",
    "                    file_name = data[0]\n",
    "                    x1,y1,x2,y2 = map(int, data[1:])\n",
    "                    \n",
    "                    img = io.imread(self.base_dir + file_name,as_grey=True)\n",
    "                    h,w = img.shape[:2]\n",
    "                    imgg = img[y1:y2,x1:x2]\n",
    "                    imgg = resize(imgg, (self.win_size, self.win_size))\n",
    "                    imgg_hog = hog(imgg)\n",
    "                    \n",
    "                    label = ord(file_name.split('/')[1][0])\n",
    "                    \n",
    "                    test_x_pos.append(imgg_hog)\n",
    "                    test_y.append(label)\n",
    "                    \n",
    "                    count = 0\n",
    "                    \n",
    "                    A = [x1,y1,x2,y2]\n",
    "                    \n",
    "                    while True:\n",
    "                        x1_r = random.randrange(0,w - self.win_size)\n",
    "                        y1_r = random.randrange(0,h - self.win_size)\n",
    "                        x2_r = x1_r + self.win_size\n",
    "                        y2_r = y1_r + self.win_size\n",
    "                        \n",
    "                        if y2_r >= h or x2_r>=w:\n",
    "                            continue\n",
    "                        \n",
    "                        B = [x1_r, y1_r, x2_r, y2_r]\n",
    "                        \n",
    "                        if self.IOU(A,B) <= 0.5:\n",
    "                            test_x_neg.append(hog(img[y1_r:y2_r,x1_r:x2_r]))\n",
    "                            count += 1\n",
    "                        \n",
    "                        if count >= 2:\n",
    "                            break\n",
    "        \n",
    "        \n",
    "        test_x_pos = np.asarray(test_x_pos)\n",
    "        test_x_neg = np.asarray(test_x_neg)\n",
    "        test_y = np.asarray(test_y)\n",
    "        \n",
    "        score_gesture = self.clf_gesture.score(test_x_pos, test_y)\n",
    "        print 'Testing accuracy for gesture classifier : %f' %(score_gesture)\n",
    "        \n",
    "        \n",
    "        test_x_hnh = np.concatenate((test_x_pos, test_x_neg))\n",
    "        test_y_hnh = np.asarray( [1] * len(test_x_pos) + [0] * len(test_x_neg) )\n",
    "        \n",
    "        score_hnh = self.clf_hnh.score(test_x_hnh,test_y_hnh)\n",
    "        print 'Testing accuracy for Hand/Non-hand classifier : %f' %(score_hnh)\n",
    "\n",
    "    \n",
    "    def hard_negative_mining(self, no_iter, threshold):\n",
    "        \n",
    "        if self.clf_hnh is None or self.clf_gesture is None:\n",
    "            print 'Classifiers not trained'\n",
    "            return\n",
    "        \n",
    "        for i in xrange(no_iter):\n",
    "            count = 0\n",
    "            FP = []\n",
    "            \n",
    "            for data in self.train_x_neg:\n",
    "                if self.clf_hnh.predict([data])[0] == 1:\n",
    "                    count+=1\n",
    "                    FP.append(data)\n",
    "            \n",
    "            if i%10==0:\n",
    "                print i, count\n",
    "                \n",
    "            if count <= threshold:\n",
    "                break\n",
    "            \n",
    "            self.clf_hnh.partial_fit(np.asarray(FP), np.asarray([0] * len(FP)))\n",
    "        \n",
    "        Y = np.asarray([1] * self.train_x_pos.shape[0] +  [0] * self.train_x_neg.shape[0])\n",
    "        score_ = self.clf_hnh.score(np.concatenate((self.train_x_pos, self.train_x_neg)), Y)\n",
    "        print 'Accuracy for Hand/Non-hand classifier after Hard Negative Mining : %f' %(score_)\n",
    "        \n",
    "    def sliding_window(self, img):\n",
    "        # conf_map = np.zeros(img.shape)\n",
    "        \n",
    "        h,w = img.shape[:2]\n",
    "        stride = 10\n",
    "        win_size = self.win_size\n",
    "        \n",
    "        max_class = 0\n",
    "        x1 = 0\n",
    "        y1 = 0\n",
    "        \n",
    "        # Y is for rows and X is for the column\n",
    "        \n",
    "        print 'Dimension test : ', (h-win_size > 10)  \n",
    "        \n",
    "        for row in xrange(0,h-win_size+1,stride):\n",
    "            for col in xrange(0,w-win_size+1,stride):\n",
    "                imgg = img[row:row+win_size, col:col+win_size]\n",
    "                hog_ = hog(imgg)\n",
    "                \n",
    "                # 1th index is giving the prob. of the class 'hand'\n",
    "                class_ = self.clf_hnh.predict_proba(np.asarray([hog_]))[0][1]\n",
    "                print 'data : ', row, col, class_\n",
    "                if class_ > max_class:\n",
    "                    max_class = class_\n",
    "                    x1 = row\n",
    "                    y1 = col\n",
    "                    \n",
    "                    # print 'max_class, x1, y1 : ', max_class, x1, y1\n",
    "        \n",
    "        print          \n",
    "        return max_class, x1, y1\n",
    "    \n",
    "    \n",
    "    def non_maximal_supression(self, boxes, overlapThresh):\n",
    "        if len(boxes) == 0:\n",
    "            return []\n",
    "        \n",
    "        pick = []\n",
    "        x1 = boxes[:,0]\n",
    "        y1 = boxes[:,1]\n",
    "        x2 = boxes[:,2]\n",
    "        y2 = boxes[:,3]\n",
    "    \n",
    "        area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "        idxs = np.argsort(y2)\n",
    "    \n",
    "        while len(idxs) > 0:\n",
    "            last = len(idxs) - 1\n",
    "            i = idxs[last]\n",
    "            pick.append(i)\n",
    "            suppress = [last]\n",
    "            for pos in xrange(0, last):\n",
    "                j = idxs[pos]\n",
    "                xx1 = max(x1[i], x1[j])\n",
    "                yy1 = max(y1[i], y1[j])\n",
    "                xx2 = min(x2[i], x2[j])\n",
    "                yy2 = min(y2[i], y2[j])\n",
    "            \n",
    "                w = max(0, xx2 - xx1 + 1)\n",
    "                h = max(0, yy2 - yy1 + 1)\n",
    "            \n",
    "                overlap = float(w * h) / area[j]\n",
    "            \n",
    "                if overlap > overlapThresh:\n",
    "                    suppress.append(pos)\n",
    "            \n",
    "            idxs = np.delete(idxs, suppress)\n",
    "\n",
    "        return boxes[pick]\n",
    "    \n",
    "    \n",
    "    def get_bbox(self, image):\n",
    "        \n",
    "        if self.clf_hnh is None or self.clf_gesture is None:\n",
    "            print 'Classifiers not trained'\n",
    "            return\n",
    "        \n",
    "        downscale_ = 1.2\n",
    "        py = pyramid_gaussian(image, downscale = downscale_)\n",
    "        py_img  = [py.next(), py.next(), py.next()]\n",
    "        \n",
    "        bbox = []\n",
    "        for i in xrange(len(py_img)):\n",
    "            c_val, row, col = self.sliding_window(py_img[i])\n",
    "            factor = downscale_**i\n",
    "            print 'scaling of points', i, factor, c_val\n",
    "            bbox.append(np.asarray(map(lambda x : int(x*factor), [row, col, row+128, col+128])))\n",
    "        \n",
    "        print 'bbox' , bbox\n",
    "        \n",
    "        pos = self.non_maximal_supression(np.asarray(bbox), 0.7)\n",
    "        \n",
    "        print 'pos len', len(pos)\n",
    "        \n",
    "        if len(pos) > 1 :\n",
    "            return pos[0]\n",
    "        return pos\n",
    "        \n",
    "    def recognize_gesture(self, image):\n",
    "\n",
    "        \"\"\"\n",
    "            image : a 320x240 pixel RGB image in the form of a numpy array\n",
    "            \n",
    "            This function should locate the hand and classify the gesture.\n",
    "\n",
    "            returns : (position, label)\n",
    "\n",
    "            position : a tuple of (x1,y1,x2,y2) coordinates of bounding box\n",
    "                       x1,y1 is top left corner, x2,y2 is bottom right\n",
    "\n",
    "            label : a single character. eg 'A' or 'B'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            position = self.get_bbox(image)\n",
    "            imgg = image[position[0]:position[2], position[1] : position[3]]\n",
    "            imgg = resize(imgg, (self.win_size, self.win_size))\n",
    "            label = chr(self.clf_gesture.predict(np.asarray( [hog(imgg)]))[0])\n",
    "            print 'POS : ', position, 'label : ', label\n",
    "            return position, label\n",
    "        except Exception as e:\n",
    "            print e\n",
    "\n",
    "    def translate_video(self, image_array):\n",
    "\n",
    "        \"\"\"\n",
    "            image_array : a list of images as described above.\n",
    "                          can be of arbitrary length\n",
    "\n",
    "            This function classifies the video into a 5 character string\n",
    "\n",
    "            returns : word (a string of 5 characters)\n",
    "                    no two consecutive characters are identical\n",
    "        \"\"\"\n",
    "\n",
    "        return word\n",
    "    \n",
    "    def test_labelled_images(self):\n",
    "        \n",
    "        if self.clf_hnh is None or self.clf_gesture is None:\n",
    "            print 'Classifiers not trained'\n",
    "            return\n",
    "\n",
    "        # test_list = [3,4,5,6,7,9,10,11,12,13,14,15,16,17,18,19]\n",
    "        test_list = [3,4,5,6,7]\n",
    "        test_list = map(lambda x : 'user_' + str(x), test_list)\n",
    "        \n",
    "        count = 0\n",
    "        total = 0\n",
    "        for user in test_list:\n",
    "            no_user = 0\n",
    "            IOU_val = 0.0\n",
    "            print user\n",
    "            \n",
    "            csv_file = self.base_dir + user + '/' + user + '_loc.csv'\n",
    "            with open(csv_file,'r') as f:\n",
    "                f.readline()\n",
    "                for line in f:\n",
    "                    data = line.strip().split(',')\n",
    "                    file_name = data[0]\n",
    "                    x1,y1,x2,y2 = map(int, data[1:])\n",
    "                    \n",
    "                    img = io.imread(self.base_dir + file_name,as_grey=True)\n",
    "                    total+=1\n",
    "                    h,w = img.shape[:2]\n",
    "                    label = file_name.split('/')[1][0]\n",
    "                    \n",
    "                    pos, label_ = self.recognize_gesture(img)\n",
    "                    \n",
    "                    IOU_val += self.IOU([x1,y1,x2,y2],pos)\n",
    "                    no_user += 1\n",
    "                    \n",
    "                    if label == label_:\n",
    "                        count+=1\n",
    "            \n",
    "            print user, 'IOU : ',IOU_val/no_user\n",
    "        \n",
    "        score_gesture = (count*1.0)/(total*1.0)\n",
    "        print 'Testing accuracy for gesture classifier class : %f' %(score_gesture)\n",
    "        \n",
    "    def store_clfs(self):\n",
    "        label = time.ctime().split()[3]\n",
    "        path = self.base_dir + 'clf_models/' + label\n",
    "        joblib.dump(self.clf_hnh, path + '_hnh.pkl')\n",
    "        joblib.dump(self.clf_gesture, path + '_gesture.pkl')\n",
    "    \n",
    "    def load_clfs(self, label):\n",
    "        path = self.base_dir + 'clf_models/' + label\n",
    "        self.clf_hnh = joblib.load(path + '_hnh.pkl')\n",
    "        self.clf_gesture = joblib.load(path + '_gesture.pkl')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ayush/GPU_ML/ML/workspace/project/complete_data/dataset/\n"
     ]
    }
   ],
   "source": [
    "G = GestureRecognizer(os.path.abspath('.') + '/dataset/')\n",
    "print G.base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G.load_clfs('06:18:36')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension test :  True\n",
      "data :  0 0 0.0\n",
      "data :  0 10 0.0\n",
      "data :  0 20 0.0\n",
      "data :  0 30 0.0\n",
      "data :  0 40 0.0\n",
      "data :  0 50 0.0\n",
      "data :  0 60 0.0\n",
      "data :  0 70 0.0\n",
      "data :  0 80 0.0\n",
      "data :  0 90 0.0\n",
      "data :  0 100 0.0\n",
      "data :  0 110 0.0\n",
      "data :  0 120 4.89623240891e-271\n",
      "data :  0 130 1.72112055382e-248\n",
      "data :  0 140 1.5007531806e-92\n",
      "data :  0 150 1.0\n",
      "data :  0 160 1.0\n",
      "data :  0 170 1.0\n",
      "data :  0 180 0.999999999854\n",
      "data :  0 190 2.16487129025e-305\n",
      "data :  10 0 0.0\n",
      "data :  10 10 0.0\n",
      "data :  10 20 0.0\n",
      "data :  10 30 0.0\n",
      "data :  10 40 0.0\n",
      "data :  10 50 0.0\n",
      "data :  10 60 0.0\n",
      "data :  10 70 0.0\n",
      "data :  10 80 0.0\n",
      "data :  10 90 0.0\n",
      "data :  10 100 0.0\n",
      "data :  10 110 0.0\n",
      "data :  10 120 0.0\n",
      "data :  10 130 8.20718706894e-218\n",
      "data :  10 140 1.0\n",
      "data :  10 150 1.0\n",
      "data :  10 160 1.0\n",
      "data :  10 170 1.0\n",
      "data :  10 180 1.0\n",
      "data :  10 190 0.0\n",
      "data :  20 0 0.0\n",
      "data :  20 10 0.0\n",
      "data :  20 20 0.0\n",
      "data :  20 30 0.0\n",
      "data :  20 40 0.0\n",
      "data :  20 50 0.0\n",
      "data :  20 60 0.0\n",
      "data :  20 70 0.0\n",
      "data :  20 80 0.0\n",
      "data :  20 90 0.0\n",
      "data :  20 100 0.0\n",
      "data :  20 110 0.0\n",
      "data :  20 120 0.0\n",
      "data :  20 130 0.0\n",
      "data :  20 140 1.0\n",
      "data :  20 150 1.0\n",
      "data :  20 160 1.0\n",
      "data :  20 170 1.0\n",
      "data :  20 180 1.30905570193e-146\n",
      "data :  20 190 0.0\n",
      "data :  30 0 0.0\n",
      "data :  30 10 0.0\n",
      "data :  30 20 0.0\n",
      "data :  30 30 0.0\n",
      "data :  30 40 0.0\n",
      "data :  30 50 0.0\n",
      "data :  30 60 0.0\n",
      "data :  30 70 0.0\n",
      "data :  30 80 0.0\n",
      "data :  30 90 0.0\n",
      "data :  30 100 0.0\n",
      "data :  30 110 0.0\n",
      "data :  30 120 0.0\n",
      "data :  30 130 0.0\n",
      "data :  30 140 0.0\n",
      "data :  30 150 4.61823255245e-84\n",
      "data :  30 160 2.93825276404e-123\n",
      "data :  30 170 8.19496335198e-222\n",
      "data :  30 180 0.0\n",
      "data :  30 190 0.0\n",
      "data :  40 0 0.0\n",
      "data :  40 10 0.0\n",
      "data :  40 20 0.0\n",
      "data :  40 30 0.0\n",
      "data :  40 40 0.0\n",
      "data :  40 50 0.0\n",
      "data :  40 60 0.0\n",
      "data :  40 70 0.0\n",
      "data :  40 80 0.0\n",
      "data :  40 90 0.0\n",
      "data :  40 100 0.0\n",
      "data :  40 110 0.0\n",
      "data :  40 120 0.0\n",
      "data :  40 130 0.0\n",
      "data :  40 140 0.0\n",
      "data :  40 150 0.0\n",
      "data :  40 160 0.0\n",
      "data :  40 170 0.0\n",
      "data :  40 180 1.16221775407e-283\n",
      "data :  40 190 0.0\n",
      "data :  50 0 0.0\n",
      "data :  50 10 0.0\n",
      "data :  50 20 0.0\n",
      "data :  50 30 0.0\n",
      "data :  50 40 0.0\n",
      "data :  50 50 0.0\n",
      "data :  50 60 0.0\n",
      "data :  50 70 0.0\n",
      "data :  50 80 0.0\n",
      "data :  50 90 0.0\n",
      "data :  50 100 0.0\n",
      "data :  50 110 0.0\n",
      "data :  50 120 0.0\n",
      "data :  50 130 0.0\n",
      "data :  50 140 0.0\n",
      "data :  50 150 0.0\n",
      "data :  50 160 0.0\n",
      "data :  50 170 2.10213148558e-221\n",
      "data :  50 180 5.58883681671e-198\n",
      "data :  50 190 0.0\n",
      "data :  60 0 0.0\n",
      "data :  60 10 0.0\n",
      "data :  60 20 0.0\n",
      "data :  60 30 0.0\n",
      "data :  60 40 0.0\n",
      "data :  60 50 0.0\n",
      "data :  60 60 0.0\n",
      "data :  60 70 0.0\n",
      "data :  60 80 0.0\n",
      "data :  60 90 0.0\n",
      "data :  60 100 0.0\n",
      "data :  60 110 0.0\n",
      "data :  60 120 0.0\n",
      "data :  60 130 0.0\n",
      "data :  60 140 0.0\n",
      "data :  60 150 0.0\n",
      "data :  60 160 0.0\n",
      "data :  60 170 0.0\n",
      "data :  60 180 3.02528176127e-09\n",
      "data :  60 190 3.45382798128e-161\n",
      "data :  70 0 0.0\n",
      "data :  70 10 0.0\n",
      "data :  70 20 0.0\n",
      "data :  70 30 0.0\n",
      "data :  70 40 0.0\n",
      "data :  70 50 0.0\n",
      "data :  70 60 0.0\n",
      "data :  70 70 0.0\n",
      "data :  70 80 0.0\n",
      "data :  70 90 0.0\n",
      "data :  70 100 0.0\n",
      "data :  70 110 0.0\n",
      "data :  70 120 0.0\n",
      "data :  70 130 0.0\n",
      "data :  70 140 0.0\n",
      "data :  70 150 0.0\n",
      "data :  70 160 0.0\n",
      "data :  70 170 0.0\n",
      "data :  70 180 6.25937550308e-94\n",
      "data :  70 190 2.0972079653e-232\n",
      "data :  80 0 0.0\n",
      "data :  80 10 0.0\n",
      "data :  80 20 0.0\n",
      "data :  80 30 0.0\n",
      "data :  80 40 0.0\n",
      "data :  80 50 0.0\n",
      "data :  80 60 0.0\n",
      "data :  80 70 0.0\n",
      "data :  80 80 0.0\n",
      "data :  80 90 0.0\n",
      "data :  80 100 0.0\n",
      "data :  80 110 0.0\n",
      "data :  80 120 0.0\n",
      "data :  80 130 0.0\n",
      "data :  80 140 0.0\n",
      "data :  80 150 3.93369500509e-201\n",
      "data :  80 160 0.0\n",
      "data :  80 170 0.0\n",
      "data :  80 180 0.0\n",
      "data :  80 190 0.0\n",
      "data :  90 0 0.0\n",
      "data :  90 10 0.0\n",
      "data :  90 20 0.0\n",
      "data :  90 30 0.0\n",
      "data :  90 40 0.0\n",
      "data :  90 50 0.0\n",
      "data :  90 60 0.0\n",
      "data :  90 70 0.0\n",
      "data :  90 80 0.0\n",
      "data :  90 90 0.0\n",
      "data :  90 100 0.0\n",
      "data :  90 110 0.0\n",
      "data :  90 120 0.0\n",
      "data :  90 130 0.0\n",
      "data :  90 140 0.0\n",
      "data :  90 150 0.0\n",
      "data :  90 160 0.0\n",
      "data :  90 170 0.0\n",
      "data :  90 180 0.0\n",
      "data :  90 190 0.0\n",
      "data :  100 0 0.0\n",
      "data :  100 10 0.0\n",
      "data :  100 20 0.0\n",
      "data :  100 30 0.0\n",
      "data :  100 40 0.0\n",
      "data :  100 50 0.0\n",
      "data :  100 60 0.0\n",
      "data :  100 70 0.0\n",
      "data :  100 80 0.0\n",
      "data :  100 90 0.0\n",
      "data :  100 100 0.0\n",
      "data :  100 110 0.0\n",
      "data :  100 120 0.0\n",
      "data :  100 130 0.0\n",
      "data :  100 140 0.0\n",
      "data :  100 150 0.0\n",
      "data :  100 160 0.0\n",
      "data :  100 170 0.0\n",
      "data :  100 180 0.0\n",
      "data :  100 190 0.0\n",
      "data :  110 0 0.0\n",
      "data :  110 10 0.0\n",
      "data :  110 20 0.0\n",
      "data :  110 30 0.0\n",
      "data :  110 40 0.0\n",
      "data :  110 50 0.0\n",
      "data :  110 60 0.0\n",
      "data :  110 70 0.0\n",
      "data :  110 80 0.0\n",
      "data :  110 90 0.0\n",
      "data :  110 100 0.0\n",
      "data :  110 110 0.0\n",
      "data :  110 120 0.0\n",
      "data :  110 130 0.0\n",
      "data :  110 140 0.0\n",
      "data :  110 150 0.0\n",
      "data :  110 160 0.0\n",
      "data :  110 170 0.0\n",
      "data :  110 180 0.0\n",
      "data :  110 190 0.0\n",
      "\n",
      "scaling of points 0 1.0 1.0\n",
      "Dimension test :  True\n",
      "data :  0 0 0.0\n",
      "data :  0 10 0.0\n",
      "data :  0 20 0.0\n",
      "data :  0 30 0.0\n",
      "data :  0 40 0.0\n",
      "data :  0 50 0.0\n",
      "data :  0 60 0.0\n",
      "data :  0 70 0.0\n",
      "data :  0 80 0.0\n",
      "data :  0 90 0.0\n",
      "data :  0 100 1.0\n",
      "data :  0 110 1.0\n",
      "data :  0 120 1.0\n",
      "data :  0 130 1.0\n",
      "data :  10 0 0.0\n",
      "data :  10 10 0.0\n",
      "data :  10 20 0.0\n",
      "data :  10 30 0.0\n",
      "data :  10 40 0.0\n",
      "data :  10 50 0.0\n",
      "data :  10 60 0.0\n",
      "data :  10 70 0.0\n",
      "data :  10 80 0.0\n",
      "data :  10 90 0.0\n",
      "data :  10 100 1.08969689684e-205\n",
      "data :  10 110 1.0\n",
      "data :  10 120 1.0\n",
      "data :  10 130 1.0\n",
      "data :  20 0 0.0\n",
      "data :  20 10 0.0\n",
      "data :  20 20 0.0\n",
      "data :  20 30 0.0\n",
      "data :  20 40 0.0\n",
      "data :  20 50 0.0\n",
      "data :  20 60 0.0\n",
      "data :  20 70 0.0\n",
      "data :  20 80 0.0\n",
      "data :  20 90 0.0\n",
      "data :  20 100 0.0\n",
      "data :  20 110 0.0\n",
      "data :  20 120 1.06863208281e-259\n",
      "data :  20 130 1.0\n",
      "data :  30 0 0.0\n",
      "data :  30 10 0.0\n",
      "data :  30 20 0.0\n",
      "data :  30 30 0.0\n",
      "data :  30 40 0.0\n",
      "data :  30 50 0.0\n",
      "data :  30 60 0.0\n",
      "data :  30 70 0.0\n",
      "data :  30 80 0.0\n",
      "data :  30 90 0.0\n",
      "data :  30 100 0.0\n",
      "data :  30 110 0.0\n",
      "data :  30 120 0.0\n",
      "data :  30 130 4.03075080808e-234\n",
      "data :  40 0 0.0\n",
      "data :  40 10 0.0\n",
      "data :  40 20 0.0\n",
      "data :  40 30 0.0\n",
      "data :  40 40 0.0\n",
      "data :  40 50 0.0\n",
      "data :  40 60 0.0\n",
      "data :  40 70 0.0\n",
      "data :  40 80 0.0\n",
      "data :  40 90 0.0\n",
      "data :  40 100 0.0\n",
      "data :  40 110 0.0\n",
      "data :  40 120 0.0\n",
      "data :  40 130 0.0\n",
      "data :  50 0 0.0\n",
      "data :  50 10 0.0\n",
      "data :  50 20 0.0\n",
      "data :  50 30 0.0\n",
      "data :  50 40 0.0\n",
      "data :  50 50 0.0\n",
      "data :  50 60 0.0\n",
      "data :  50 70 0.0\n",
      "data :  50 80 0.0\n",
      "data :  50 90 0.0\n",
      "data :  50 100 0.0\n",
      "data :  50 110 0.0\n",
      "data :  50 120 0.0\n",
      "data :  50 130 4.30774801844e-262\n",
      "data :  60 0 0.0\n",
      "data :  60 10 0.0\n",
      "data :  60 20 0.0\n",
      "data :  60 30 0.0\n",
      "data :  60 40 0.0\n",
      "data :  60 50 0.0\n",
      "data :  60 60 0.0\n",
      "data :  60 70 0.0\n",
      "data :  60 80 0.0\n",
      "data :  60 90 0.0\n",
      "data :  60 100 0.0\n",
      "data :  60 110 0.0\n",
      "data :  60 120 0.0\n",
      "data :  60 130 0.0\n",
      "data :  70 0 0.0\n",
      "data :  70 10 0.0\n",
      "data :  70 20 0.0\n",
      "data :  70 30 0.0\n",
      "data :  70 40 0.0\n",
      "data :  70 50 0.0\n",
      "data :  70 60 0.0\n",
      "data :  70 70 0.0\n",
      "data :  70 80 0.0\n",
      "data :  70 90 0.0\n",
      "data :  70 100 0.0\n",
      "data :  70 110 0.0\n",
      "data :  70 120 0.0\n",
      "data :  70 130 0.0\n",
      "\n",
      "scaling of points 1 1.2 1.0\n",
      "Dimension test :  True\n",
      "data :  0 0 0.0\n",
      "data :  0 10 0.0\n",
      "data :  0 20 0.0\n",
      "data :  0 30 0.0\n",
      "data :  0 40 0.0\n",
      "data :  0 50 0.0\n",
      "data :  0 60 0.0\n",
      "data :  0 70 0.0\n",
      "data :  0 80 5.81045229516e-148\n",
      "data :  0 90 1.0\n",
      "data :  10 0 0.0\n",
      "data :  10 10 0.0\n",
      "data :  10 20 0.0\n",
      "data :  10 30 0.0\n",
      "data :  10 40 0.0\n",
      "data :  10 50 0.0\n",
      "data :  10 60 0.0\n",
      "data :  10 70 0.0\n",
      "data :  10 80 0.0\n",
      "data :  10 90 0.0\n",
      "data :  20 0 0.0\n",
      "data :  20 10 0.0\n",
      "data :  20 20 0.0\n",
      "data :  20 30 0.0\n",
      "data :  20 40 0.0\n",
      "data :  20 50 0.0\n",
      "data :  20 60 0.0\n",
      "data :  20 70 0.0\n",
      "data :  20 80 0.0\n",
      "data :  20 90 0.0\n",
      "data :  30 0 0.0\n",
      "data :  30 10 0.0\n",
      "data :  30 20 0.0\n",
      "data :  30 30 0.0\n",
      "data :  30 40 0.0\n",
      "data :  30 50 0.0\n",
      "data :  30 60 0.0\n",
      "data :  30 70 0.0\n",
      "data :  30 80 0.0\n",
      "data :  30 90 0.0\n",
      "\n",
      "scaling of points 2 1.44 1.0\n",
      "bbox [array([  0, 150, 128, 278]), array([  0, 120, 153, 273]), array([  0, 129, 184, 313])]\n",
      "pos len 1\n",
      "index 2 is out of bounds for axis 0 with size 1\n"
     ]
    }
   ],
   "source": [
    "img = io.imread('/home/ayush/GPU_ML/ML/workspace/project/complete_data/dataset/user_3/B0.jpg', as_grey=True)\n",
    "# print img.shape\n",
    "G.recognize_gesture(img)\n",
    "# pos, label = \n",
    "# print pos, label\n",
    "# io.imshow(img[pos[0]:pos[2],pos[1]:pos[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.21775349e-150,   1.00000000e+000])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.clf_hnh.predict_proba([hog(resize(img[139:289,12:162], (128,128)))])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8175624798307577"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.clf_hnh.classes_\n",
    "G.IOU([139,12,289,162], [130, 0, 296, 166])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user_3', 'user_4', 'user_5', 'user_6', 'user_7', 'user_9', 'user_10', 'user_11', 'user_12', 'user_13', 'user_14', 'user_15']\n",
      "['user_16', 'user_17', 'user_18', 'user_19']\n"
     ]
    }
   ],
   "source": [
    "user = [3,4,5,6,7,9,10,11,12,13,14,15]\n",
    "user = map(lambda x : 'user_' + str(x), user)\n",
    "print user\n",
    "\n",
    "user_test = [16,17,18,19]\n",
    "user_test = map(lambda x : 'user_' + str(x), user_test)\n",
    "print user_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for gesture classifier : 1.000000\n",
      "Training accuracy for Hand/Non-hand classifier : 0.941551\n"
     ]
    }
   ],
   "source": [
    "G.train(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 410\n",
      "10 105\n",
      "20 66\n",
      "30 35\n",
      "40 25\n",
      "50 18\n",
      "60 11\n",
      "70 6\n",
      "Accuracy for Hand/Non-hand classifier after Hard Negative Mining : 0.977199\n"
     ]
    }
   ],
   "source": [
    "G.hard_negative_mining(100,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy for gesture classifier : 0.764583\n",
      "Testing accuracy for Hand/Non-hand classifier : 0.951736\n"
     ]
    }
   ],
   "source": [
    "G.test(user_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G.store_clfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "io.imshow(img[,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_3\n",
      "index 2 is out of bounds for axis 0 with size 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-43d577bd9848>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_labelled_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-b74c58c00609>\u001b[0m in \u001b[0;36mtest_labelled_images\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    358\u001b[0m                     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                     \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecognize_gesture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                     \u001b[0mIOU_val\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIOU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "G.test_labelled_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
